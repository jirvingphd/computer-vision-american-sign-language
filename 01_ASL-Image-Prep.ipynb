{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Sign Language - Computer Vision Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset: https://public.roboflow.com/object-detection/american-sign-language-letters\n",
    "- Example Task: https://towardsdatascience.com/sign-language-recognition-with-advanced-computer-vision-7b74f20f3442"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERRIDE_TESTING = False # Set to True to run tests even if not in testing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the seed for TensorFlow\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# sys.path.append(os.path.abspath(\"../../\"))\n",
    "import custom_functions as cf\n",
    "help(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from pprint import pprint\n",
    "\n",
    "# Define filename for project config filepaths json file\n",
    "FPATHS_FILE = \"config/filepaths.json\"\n",
    "os.makedirs(os.path.dirname(FPATHS_FILE), exist_ok=True)\n",
    "\n",
    "# Define Filepaths\n",
    "FPATHS = dict(\n",
    "    data={\n",
    "        # Images Directoryies\n",
    "        'data_dir': \"./American Sign Language Letters.v1-v1.multiclass/\",\n",
    "        \"train-images_dir\": \"./American Sign Language Letters.v1-v1.multiclass/train/\",\n",
    "        \"test-images_dir\": \"./American Sign Language Letters.v1-v1.multiclass/test/\",\n",
    "        \n",
    "        # Image classes as csv fiels\n",
    "        \"train-labels_csv\": \"./American Sign Language Letters.v1-v1.multiclass/train/_classes.csv\",\n",
    "        \"test-labels_csv\": \"./American Sign Language Letters.v1-v1.multiclass/test/_classes.csv\",\n",
    "        \n",
    "        # Processed versions of the above csv files\n",
    "        \"train-labels_processed_csv\": \"./American Sign Language Letters.v1-v1.multiclass/train/_classes_processed.csv\",\n",
    "        \"test-labels_processed_csv\": \"./American Sign Language Letters.v1-v1.multiclass/test/_classes_processed.csv\",\n",
    "\n",
    "        },\n",
    "\n",
    "    images={\n",
    "        \"banner\": \"images/American_Sign_Language_ASL.svg\",\n",
    "    },\n",
    "    # Any images to be displayed in the app\n",
    "    eda={\n",
    "        \"label-distrubtion-countplot_png\": \"images/label-distribution-countplot.png\",\n",
    "        \"test-labels-distrubtion_png\": \"images/test-label-distribution-countplot.png\",\n",
    "        \"example-images_png\": \"images/ed_example_letters.png\",\n",
    "    },\n",
    "    modeling={\n",
    "            \"train-dataset_dir\": \"modeling/data/training-data-tf/\",  # train_ds\n",
    "            \"val-dataset_dir\": \"modeling/data/validation-data-tf/\",  # train_ds\n",
    "            \"test-dataset_dir\": \"modeling/data/testing-data-tf/\",  # test_ds\n",
    "            \"params\":\"modeling/params.json\",\n",
    "            \"best_cnn_fpaths\": {'model_dir': \"modeling/models/best_cnn/\",\n",
    "                                      \"results_dir\": \"modeling/models/best_cnn/results/\",\n",
    "                                      'model_history_png': \"modeling/models/best_cnn/results/model_history.png\",\n",
    "                                        \"model_confusion_matrix\": \"modeling/models/best_cnn/results/model_confusion_matrix.png\",\n",
    "                                        \"model_classification_report\": \"modeling/models/best_cnn/results/model_classification_report.png\",\n",
    "                                  },\n",
    "            # 'best_model_cnn_dir': \"modeling/models/best_cnn/\",\n",
    "            \"best_transfer_fpaths\": {'model_dir': \"modeling/models/transfer_learning/\",\n",
    "                                                    \"results_dir\": \"modeling/models/transfer_learning/results/\",\n",
    "                                                    \"model_history_png\": \"modeling/models/transfer_learning/results/model_history.png\",\n",
    "                                                    \"model_confusion_matrix\": \"modeling/models/transfer_learning/results/model_confusion_matrix.png\",\n",
    "                                                    \"model_classification_report\": \"modeling/models/transfer_learning/results/model_classification_report.png\",\n",
    "            },\n",
    "                                                    \n",
    "            # 'transfer_learning_dir': \"modeling/models/transfer_learning/\",\n",
    "            \"label-lookup_json\": \"modeling/label_lookup.json\",\n",
    "},\n",
    "    config = {'log_fpath': \"logs/nn_training.log\",}\n",
    "    # results={\"results_dir\": \"results/\",\n",
    "    #          'best_model_cnn_dir': \"results/best_cnn/\",\n",
    "    #          \"best_model_cnn_history\": \"results/best_cnn/history.png\",\n",
    "    #          \"best_model_cnn_confusion_matrix\": \"results/best_cnn/confusion_matrix.png\",\n",
    "    #          \"best_model_cnn_classification_report\": \"results/best_cnn/classification_report.png\",\n",
    "             \n",
    "    #          \"transfer_learning_dir\": \"results/transfer_learning/\",\n",
    "    #          \"transfer_learning_history\": \"results/transfer_learning/history.png\",\n",
    "    #          \"transfer_learning_confusion_matrix\": \"results/transfer_learning/confusion_matrix.png\",\n",
    "    #             \"transfer_learning_classification_report\": \"results/transfer_learning/classification_report.png\",\n",
    "    #             },\n",
    ")\n",
    "FPATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use fn for local package, ds for pip version\n",
    "cf.utils.create_directories_from_paths(FPATHS)\n",
    "# ds.utils.create_directories_from_paths(FPATHS)\n",
    "\n",
    "print('[i] FPATHS Dictionary:\\n')\n",
    "pprint(FPATHS.keys())#, indent=4)\n",
    "\n",
    "## Save the filepaths\n",
    "with open(FPATHS_FILE, \"w\") as f:\n",
    "    json.dump(FPATHS, f)\n",
    "    print(f\"\\n[i] Saved FPATHS to {FPATHS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Project Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set project-wide parameters\n",
    "OVERWRITE_LOGS = True\n",
    "\n",
    "# # Saving image params as vars for reuse\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "\n",
    "## Set data split proportions\n",
    "TRAIN_SPLIT = 0.7  # Proportion of data for training\n",
    "VAL_SPLIT = 0.15  # Proportion of data for validation (remaining will be for test)\n",
    "\n",
    "\n",
    "# Save model params\n",
    "PATIENCE = 10  # For early stopping\n",
    "EPOCHS = 2#0  # Max number of epochs to run\n",
    "print(f\"EPOCHS TEMPORARILY SET TO {EPOCHS}\")\n",
    "\n",
    "import json\n",
    "## Save model params from above to json\n",
    "params = {\"BATCH_SIZE\":BATCH_SIZE,\n",
    "          \"IMG_HEIGHT\":IMG_HEIGHT,\n",
    "          \"IMG_WIDTH\":IMG_WIDTH,\n",
    "          \"TRAIN_SPLIT\":TRAIN_SPLIT,\n",
    "          \"VAL_SPLIT\":VAL_SPLIT,\n",
    "          \"PATIENCE\":PATIENCE,\n",
    "          \"EPOCHS\":EPOCHS}\n",
    "\n",
    "with open(FPATHS['modeling']['params'], \"w\") as f:\n",
    "    json.dump(params, f)\n",
    "    print(f\"\\n[i] Saved params to {FPATHS['modeling']['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the contents of data folder\n",
    "data_dir = FPATHS['data']['data_dir'] #\"./American Sign Language Letters.v1-v1.multiclass/\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of img file paths (ONLY, did not make recursuve so no folders)\n",
    "img_files = glob.glob(data_dir+\"**/*\")#, recursive=True)\n",
    "len(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview an example image (at full size)\n",
    "img_loaded = load_img(img_files[0])\n",
    "img_data = img_to_array(img_loaded)\n",
    "print(img_data.shape)\n",
    "array_to_img(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare CSV of Filenames + Labels (1 per train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "# csv_path = os.path.join(data_dir,\"train\",\"_classes.csv\")\n",
    "train_csv = FPATHS['data']['train-labels_csv']\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_train = df_train.convert_dtypes()\n",
    "df_train = df_train.set_index('filename')\n",
    "df_train = df_train.astype(float)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving list of one-hot-encoded labels\n",
    "label_cols = classes = sorted(df_train.drop(columns=['filename','filepath','label'], errors='ignore').columns)\n",
    "label_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Combine label columns into single column\n",
    "df_train.loc[:,'label'] = df_train[label_cols].apply(lambda x: x.idxmax(), axis=1)\n",
    "display(df_train.head(2))\n",
    "\n",
    "df_train['label'].value_counts(1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prepend folder path to image filenames\n",
    "train_img_dir = FPATHS['data']['train-images_dir']\n",
    "\n",
    "# Save label lookup dictionary\n",
    "label_lookup = {i:label for i,label in enumerate(classes)}\n",
    "label_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the filepaths and labels\n",
    "df_train = df_train.reset_index(drop=False)\n",
    "df_train['filepath'] = df_train.loc[:,'filename'].astype(str).map(lambda x: os.path.join(train_img_dir, x)).values\n",
    "filepaths = df_train['filepath']\n",
    "\n",
    "labels = df_train[label_cols].astype(float).values\n",
    "filepaths[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels_df(csv_fpath, img_dir, return_label_lookup=False, \n",
    "                      save_processed_csv=False, processed_csv_fpath=None,\n",
    "                      save_label_lookup=False, label_lookup_fpath=None):\n",
    "    \n",
    "    df = pd.read_csv(csv_fpath)\n",
    "    \n",
    "    df = df.convert_dtypes()\n",
    "    \n",
    "    # Save label columns\n",
    "    label_cols = classes = sorted(df.drop(columns=['filename','filepath','label'], errors='ignore').columns)\n",
    "    # df = df.set_index('filename')\n",
    "    df[label_cols] = df[label_cols].astype(float)\n",
    "    \n",
    "    # Combine labels into single column for EDA\n",
    "    df.loc[:,'label'] = df[label_cols].apply(lambda x: x.idxmax(), axis=1)\n",
    "    \n",
    "    # df = df.reset_index(drop=False)\n",
    "    \n",
    "    # Save prepend folder path to image filenames\n",
    "    df['filepath'] = df.loc[:,'filename'].astype(str).map(lambda x: os.path.join(img_dir, x))\n",
    "    \n",
    "    \n",
    "    if return_label_lookup | save_label_lookup:\n",
    "        # Save label lookup dictionary\n",
    "        label_lookup = {i:label for i,label in enumerate(classes)}\n",
    "    \n",
    "    if save_label_lookup:\n",
    "        with open(label_lookup_fpath, \"w\") as f:\n",
    "            json.dump(label_lookup, f)\n",
    "            print(f\"\\n[i] Saved label lookup to {label_lookup_fpath}\")\n",
    "\n",
    "    \n",
    "    if save_processed_csv:\n",
    "        # Save processed csv\n",
    "        df.to_csv(processed_csv_fpath, index=False)\n",
    "        print(f\"\\n[i] Saved processed csv to {processed_csv_fpath}\")\n",
    "    \n",
    "    if return_label_lookup:\n",
    "        print(\"- DataFrame and label lookup dictionary returned.\")\n",
    "        return df, label_lookup\n",
    "    \n",
    "    else:\n",
    "        print(\"- DataFrame only returned.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and labels\n",
    "df_train, label_lookup = prepare_labels_df(FPATHS['data']['train-labels_csv'], \n",
    "                             FPATHS['data']['train-images_dir'], \n",
    "                             save_label_lookup=True, label_lookup_fpath=FPATHS['modeling']['label-lookup_json'],\n",
    "                             save_processed_csv=True, processed_csv_fpath=FPATHS['data']['train-labels_processed_csv'],\n",
    "                             return_label_lookup=True)\n",
    "df_train.head(2)\n",
    "label_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and labels\n",
    "df_test = prepare_labels_df(FPATHS['data']['test-labels_csv'], \n",
    "                             FPATHS['data']['test-images_dir'], \n",
    "                             save_label_lookup=False, \n",
    "                             save_processed_csv=True, processed_csv_fpath=FPATHS['data']['test-labels_processed_csv'],\n",
    "                             return_label_lookup=False)\n",
    "display(df_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined dataframe of filepaths and labels\n",
    "# df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_exist = np.array([os.path.exists(f) for f in filepaths])\n",
    "# files_exist.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load an image and its label with keras utils\n",
    "# filepaths = df_train['filepath']\n",
    "# print(f\"Letter: {label_lookup[np.argmax(labels[0])]}\")\n",
    "# display(load_img(filepaths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images for dataset\n",
    "def load_image(filename, label, img_height=128, img_width=128):\n",
    "    img = tf.io.read_file(filename)\n",
    "    # img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img.set_shape([None, None, 3])  # Explicitly set the shape\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    # img = img / 255.0  # Normalize the image\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispay an example image\n",
    "i = 0\n",
    "# filepaths = df_train['filepath']\n",
    "# labels = df_train[label_cols].astype(float).values\n",
    "\n",
    "def preview_image(filepaths, labels, i, label_lookup, title=\"\"):\n",
    "    ex_img, ex_label = load_image(filepaths[i], labels[i])\n",
    "    \n",
    "    print(\"\\n[i] Preview Image\"+ title)\n",
    "    print(f\"- Label (Category): {label_lookup[np.argmax(ex_label)]}\")\n",
    "    print(f\"- Label (OHE): {ex_label}\")\n",
    "\n",
    "    display(array_to_img(ex_img))\n",
    "    print(f\"- Image Shape: {ex_img.shape}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# Preview an example from training and testing data\n",
    "preview_image(filepaths=df_train['filepath'].values, labels=df_train[label_cols].values, i=0, label_lookup=label_lookup,\n",
    "              title=\" (Training Data)\")\n",
    "\n",
    "preview_image(filepaths=df_test['filepath'].values, labels=df_test[label_cols].values, i=0,label_lookup=label_lookup,\n",
    "              title=\" (Testing Data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_df = df_train[['filepath', 'label']]\n",
    "# eda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_distribution(eda_df, x='label', title=\"Distribution of Labels (Training Data)\", xlabel=\"Letter\", ylabel=\"Count\",\n",
    "                            label_lookup=None, save_path=None):\n",
    "    if label_lookup is None:\n",
    "        classes = sorted(eda_df[x].unique())\n",
    "    else:\n",
    "        classes = sorted(label_lookup.values())\n",
    "    \n",
    "    # Plot Distrubtion of Labels in Training Data\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax = sns.countplot(data=eda_df, x=x,order=classes,#label_lookup.values(),\n",
    "                hue=x, dodge=False,palette=sns.color_palette(\"icefire\",n_colors=len(classes)),\n",
    "                ax=ax)\n",
    "    ax.set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight', transparent=False)\n",
    "        \n",
    "    return fig, ax\n",
    "        \n",
    "fig, ax = plot_label_distribution(df_train, title=\"Distribution of Labels (Training Data)\", save_path=FPATHS['eda']['label-distrubtion-countplot_png'],\n",
    "                                  label_lookup=label_lookup)\n",
    "fig, ax = plot_label_distribution(df_test, title=\"Distribution of Labels (Test Data)\", save_path=FPATHS['eda']['test-labels-distrubtion_png'],\n",
    "                                  \n",
    "                                  label_lookup=label_lookup)\n",
    "# FPATHS['eda']['label-distrubtion-countplot_png']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Example of Each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot example of each letter\n",
    "import os\n",
    "# os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "def plot_example_images(eda_df,label_col='label',fpath_col = \"filepath\", ncols = 6,figsize=(15,15),\n",
    "                        save_path=None, suptitle=None, suptitle_y=1.02, suptitle_fontsize=16):\n",
    "    # Save labels and determine rows\n",
    "    unique_labels = sorted(eda_df[label_col].unique())\n",
    "    nrows = len(unique_labels)//ncols + 1\n",
    "    \n",
    "    \n",
    "    ## Create figure and flatten axes\n",
    "    fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot example of each\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # Selet random example of label\n",
    "        fpath = eda_df.loc[ eda_df[label_col]==label,fpath_col].sample(1).values[0]\n",
    "        \n",
    "        # Load and plot the iamge\n",
    "        loaded = plt.imread(fpath)\n",
    "        axes[i].imshow(loaded)\n",
    "        axes[i].set_title(label)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "    # Remove unused axes    \n",
    "    axes_labels_diff =  len(axes) - len(unique_labels)\n",
    "    if axes_labels_diff>0:\n",
    "        for ax in axes[-axes_labels_diff:]:\n",
    "            fig.delaxes(ax=ax)   \n",
    "    \n",
    "    # Tweak layout\n",
    "    fig.tight_layout()\n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle, y=suptitle_y, fontsize=suptitle_fontsize)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight', transparent=False)\n",
    "        \n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "fig, axes = plot_example_images(df_train, save_path=FPATHS['eda']['example-images_png'],\n",
    "                                suptitle=\"Example Images of Each Letter in Training Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Train/Test/Val Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_image(image_paths[0], labels[0])\n",
    "\n",
    "filepaths = df_train['filepath'].values\n",
    "labels = df_train[label_cols].values\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(buffer_size=len(dataset), reshuffle_each_iteration=False)\n",
    "\n",
    "dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map the load_image function to the dataseta\n",
    "dataset = dataset.map(lambda x,y: load_image(x,y),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine split sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(TRAIN_SPLIT * total_size)\n",
    "val_size = int(VAL_SPLIT * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "print(f\"{train_size=}, {test_size=}, {val_size=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "test_dataset = dataset.skip(train_size + val_size)\n",
    "\n",
    "# Cache the datset for faster access\n",
    "train_dataset = train_dataset.cache()\n",
    "val_dataset = val_dataset.cache()\n",
    "test_dataset = test_dataset.cache() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and prefetch the datasets\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle the trainin data\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_dataset.cardinality(), \n",
    "                                      reshuffle_each_iteration=True) # DOUBLE CHECK BATCH_SIZE * 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the datasets\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(f\"Train batch - images: {images.shape}, labels: {labels.shape}\")\n",
    "    \n",
    "for images, labels in val_dataset.take(1):\n",
    "    print(f\"Val batch - images: {images.shape}, labels: {labels.shape}\")\n",
    "    \n",
    "    \n",
    "for images, labels in test_dataset.take(1):\n",
    "    print(f\"Test batch - images: {images.shape}, labels: {labels.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model (From towardsdatascience blog)\n",
    "- from https://towardsdatascience.com/sign-language-recognition-with-advanced-computer-vision-7b74f20f3442\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moedl from https://towardsdatascience.com/sign-language-recognition-with-advanced-computer-vision-7b74f20f3442\n",
    "# from tensorflow\n",
    "def make_model(name='towards-data-science',show_summary=False, use_schedule=False):\n",
    "    model = models.Sequential(name=name)\n",
    "    model.add(layers.Rescaling(1./255 , input_shape = (IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "    \n",
    "    model.add(layers.Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' ))#, input_shape = (28,28,1)))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    model.add(layers.Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    model.add(layers.Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    \n",
    "    # Final layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units = 512 , activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(units = len(label_lookup   ) , activation = 'softmax'))\n",
    "    \n",
    "    \n",
    "    ## JMI:\n",
    "    if use_schedule:\n",
    "        lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=0.01, decay_steps=10000, decay_rate=0.95\n",
    "        )  # 0.9)\n",
    "        optimizer = optimizers.legacy.Adam(learning_rate=lr_schedule)\n",
    "    else:\n",
    "        optimizer = optimizers.legacy.Adam()#learning_rate=0.01)\n",
    "        \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    # model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Demonstrate model architecture\n",
    "model = make_model(show_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `def get_callbacks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_callbacks(monitor='val_accuracy', patience=PATIENCE, #15,\n",
    "                  start_from_epoch=3, restore_best_weights=False):\n",
    "    \"\"\"\n",
    "    Returns a list of callbacks for training a model.\n",
    "\n",
    "    Parameters:\n",
    "    - monitor (str): The metric to monitor. Default is 'val_accuracy'.\n",
    "    - patience (int): The number of epochs with no improvement after which training will be stopped. Default is 15.\n",
    "    - start_from_epoch (int): The epoch from which to start counting the patience. Default is 3.\n",
    "    - restore_best_weights (bool): Whether to restore the weights of the best epoch. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - callbacks (list): A list of callbacks to be used during model training.\n",
    "    \"\"\"\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,start_from_epoch=start_from_epoch,\n",
    "                                                      monitor=monitor,\n",
    "                                                      restore_best_weights=restore_best_weights, verbose=1)\n",
    "    return [early_stopping]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define updated evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With 26 classes, it is difficult to scan the performance for each class visually. Adding code to convert results to a datafarme and use pandas styling to visualize\n",
    "\n",
    "- added new `get_results_df` to custom_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  New Custom Eval Function: \n",
    "- `custom_evaluate_classification_network` (for Notebook use only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate_classification_network(model, X_test, history=None, figsize=(15,15), target_names=None,\n",
    "                                            #  as_frame=True, \n",
    "                                             frame_include_macro_avg=True, frame_include_support=False,\n",
    "                                             display_bar=False, bar_subset_cols = ['recall','precision','f1-score'], \n",
    "                                             conf_matrix_text_kws={'fontsize': 'x-small'},\n",
    "                                             return_figs= True, return_str_report=True):\n",
    "    \"\"\"\n",
    "    Evaluate a classification model on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained classification model.\n",
    "    - X_test: The test dataset.\n",
    "    - history: The training history of the model (optional).\n",
    "    - figsize: The size of the figure for plotting the evaluation results (default: (15, 15)).\n",
    "    - target_names: The names of the target classes (default: None).\n",
    "    - as_frame: Whether to return the evaluation results as a pandas DataFrame (default: True).\n",
    "    - frame_include_macro_avg: Whether to include macro average metrics in the DataFrame (default: False).\n",
    "    - frame_include_support: Whether to include support values in the DataFrame (default: False).\n",
    "    - display_bar: Whether to display the evaluation results as a styled bar chart (default: True).\n",
    "\n",
    "    Returns:\n",
    "    - results_dict: A dictionary containing the evaluation results.\n",
    "    \"\"\"\n",
    "    if target_names is None:\n",
    "        # label_lookup is in the global scope\n",
    "        target_names = label_lookup.values()\n",
    "        \n",
    "    results_dict = cf.evaluate_classification_network(model,\n",
    "                                                      X_test=X_test,history=history, figsize=figsize,\n",
    "                                                # Set output to produce a dataframe (no option)\n",
    "                                                  output_dict=True, as_frame=True,\n",
    "                                                  target_names=target_names,\n",
    "                                                  return_fig_conf_matrix=return_figs,\n",
    "                                                  return_fig_history=return_figs,\n",
    "                                                    frame_include_macro_avg=frame_include_macro_avg, \n",
    "                                                    frame_include_support=frame_include_support,\n",
    "                                                    values_format=\".2f\",\n",
    "                                                    conf_matrix_text_kws=conf_matrix_text_kws,\n",
    "                                                    return_str_report=return_str_report)\n",
    "    \n",
    "    if isinstance(results_dict, tuple):\n",
    "        results_dict, fig_dict = results_dict\n",
    "\n",
    "        \n",
    "        if isinstance(results_dict, dict):\n",
    "            class_results = results_dict['test']['results-classes']\n",
    "            overall_results = results_dict['test']['results-overall']\n",
    "        else:\n",
    "            class_results = results_dict['test']['results-classes']\n",
    "            overall_results = None\n",
    "            # print(results_dict)\n",
    "    else:\n",
    "        raise Exception(\"Results dict not a tuple\")\n",
    "    # elif isinstance(results_dict, dict):\n",
    "    #     class_results = results_dict['test']['results-classes']\n",
    "    #     overall_results = results_dict['test']['results-overall']:\n",
    "        \n",
    "    return_list = [results_dict]\n",
    "    \n",
    "    if display_bar:\n",
    "        try:\n",
    "            plot_data = results_dict['test']['results-classes']\n",
    "            display(plot_data.style.bar(subset=bar_subset_cols, color='#5fba7d').format(formatter= lambda x: f\"{x:.2f}\").set_caption(\"Test Data\"))\n",
    "        except:\n",
    "            display(results_dict)\n",
    "        \n",
    "    if return_figs:\n",
    "        return results_dict, fig_dict\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìåBOOKMARK: Controlling Text Size on Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # TEST CODE (Must run modeling below first)\n",
    "# results = custom_evaluate_classification_network(model,X_test=test_dataset, history=history, figsize=(15,15),\n",
    "#                                                  target_names=label_lookup.values(),display_bar=True,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (with New Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show model architecture\n",
    "model = make_model(show_summary=True, use_schedule=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "def initialize_logs(log_file = FPATHS['config']['log_fpath'], overwrite_logs=True,\n",
    "                    log_header = \"start_time,name,fit_time,metrics\"):\n",
    "    # #add deleting log file\n",
    "    if overwrite_logs==True:\n",
    "        filemode = \"w\"\n",
    "        force=True\n",
    "    else:\n",
    "        filemode = 'a'\n",
    "        force = False\n",
    "        \n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, filemode=filemode,force=force)#, format='%(message)s')\n",
    "    loggin.info(log_header)\n",
    "\n",
    "initialize_logs(log_file= FPATHS['config']['log_fpath'], overwrite_logs=OVERWRITE_LOGS)\n",
    "# Function to log neural network details\n",
    "def log_nn_details(start_time, name, fit_time, results_overall):\n",
    "    \n",
    "    metrics = results_overall.loc['macro avg'].to_dict()\n",
    "    # Add date recorded \n",
    "    # date_recorded = dt.datetime.now()\n",
    "    info = f\"{start_time.strftime('%m/%d/%Y %T')},{name},{fit_time},{metrics}\"\n",
    "    # for metric,value in metrics.items():\n",
    "    #     # info += f\",{metric}:{value} \"\n",
    "    #     info += f\",{value:.2f}\"\n",
    "    info+=\"\\n\"\n",
    "    logging.info(info)\n",
    "    \n",
    "\n",
    "def fit_log_model(model, train_dataset, val_dataset, test_dataset, epochs=EPOCHS, patience=PATIENCE,fit_kws= {}, callback_kws={}):\n",
    "    # Save start time\n",
    "    \n",
    "    start_time = dt.datetime.now()\n",
    "    \n",
    "    callbacks = get_callbacks(patience=patience, **callback_kws)\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, verbose=1,\n",
    "                        callbacks=callbacks, **fit_kws)\n",
    "    fit_time = dt.datetime.now() - start_time\n",
    "\n",
    "    results_frames, results_figs = custom_evaluate_classification_network(model,X_test=test_dataset, history=history, figsize=(15,15),\n",
    "                                                 target_names=label_lookup.values(),display_bar=True,\n",
    "                                                 frame_include_macro_avg=True,frame_include_support=False);\n",
    "    print(f\"{type(results_frames)=}, {type(results_figs)=}\")\n",
    "    \n",
    "    if isinstance(results_frames, dict):\n",
    "        class_results = results_frames['test']['results-classes']\n",
    "        overall_results = results_frames['test']['results-overall']\n",
    "    else:\n",
    "        class_results = results_frames\n",
    "        overall_results = None\n",
    "        \n",
    "    if overall_results is not None:\n",
    "        print(\"\\nOverall Results:\")\n",
    "        display(overall_results)\n",
    "        log_nn_details(start_time, model.name, fit_time, overall_results)\n",
    "    \n",
    "    return dict(model=model, history=history, results_classes =class_results,\n",
    "                results_overall= overall_results, result_figs=results_figs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Baseline model\n",
    "model = make_model(show_summary=False, use_schedule=False)\n",
    "model_results = fit_log_model(model, train_dataset, val_dataset, test_dataset, epochs=EPOCHS, patience=PATIENCE)\n",
    "\n",
    "# history = model.fit(train_dataset, epochs = EPOCHS,\n",
    "#                     validation_data = val_dataset, callbacks=get_callbacks())\n",
    "# # results_dict = cf.evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "# #                                                   output_dict=True, target_names=label_lookup.values(),\n",
    "# #                                                   as_frame=True,\n",
    "# #                                                   frame_include_macro_avg=False,frame_include_support=False)\n",
    "# results, result_figs = custom_evaluate_classification_network(model,X_test=test_dataset, history=history, figsize=(15,15),\n",
    "#                                                  target_names=label_lookup.values(),display_bar=True,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bookmark: 06/01 - Parsing and Displaying/Saving DataFrame Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Parse the log file\n",
    "# def parse_log_file(log_file):\n",
    "#     data = []\n",
    "#     with open(log_file, 'r') as file:\n",
    "#         for line in file:\n",
    "#             name, fit_time, metrics = line.strip().split(',', 2)\n",
    "#             metrics = eval(metrics)  # Convert string representation of dictionary to dictionary\n",
    "#             data.append([\n",
    "#                 name, \n",
    "#                 float(fit_time), \n",
    "#                 metrics['accuracy'], \n",
    "#                 metrics['precision'], \n",
    "#                 metrics['recall'], \n",
    "#                 metrics['f1_score']\n",
    "#             ])\n",
    "            \n",
    "\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing log file parsing\n",
    "with open(log_file, 'r') as file:\n",
    "    log_lines = file.readlines()\n",
    "    # file.seek(0)\n",
    "    # log = file.read()\n",
    "\n",
    "\n",
    "    # for line in file:\n",
    "    #     print(line.strip())\n",
    "# print(log)\n",
    "log_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(log_file):\n",
    "    data = []\n",
    "    with open(log_file, 'r') as file:\n",
    "        log_lines= file.readlines()\n",
    "\n",
    "    for line in log_lines:\n",
    "        start_time, name, fit_time, metrics = line.strip().split(',')#, 2)\n",
    "        metrics = eval(metrics)  # Convert string representation of dictionary to dictionary\n",
    "        data.append([start_time, name,fit_time, metrics \n",
    "        ])\n",
    "    return data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data =parse_log_file(log_file)\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Stop here and verify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Display the information as a table\n",
    "def display_as_table(data):\n",
    "    df = pd.DataFrame(data, columns=[\"Name\", \"Fit Time (s)\", \"Accuracy\"])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = parse_log_file(FPATHS['config']['log_fpath'])\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"inspect logs above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_frames, results_figs = custom_evaluate_classification_network(model,X_test=test_dataset, history=history, figsize=(15,15),\n",
    "#                                                  target_names=label_lookup.values(),display_bar=True,);\n",
    "# if len(results_frames) > 1:\n",
    "#     class_results, overall_results = results_frames\n",
    "# results_figs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ To Do:\n",
    "- Define a save_results function to use the filepaths in FPATHS to save the results dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{type(results_frames)=}, {len(results_frames)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_frames['test'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_frames['test']['results-classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_frames['test']['results-overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save datasets\n",
    "# train_dataset.save(FPATHS['modeling']['train-dataset_dir'])\n",
    "# test_dataset.save(FPATHS['modeling']['test-dataset_dir'])\n",
    "# val_dataset.save(FPATHS['modeling']['val-dataset_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPATHS['modeling']['best_cnn_fpaths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Save Function:\n",
    "\n",
    "- Notes:\n",
    "    - Need to save:\n",
    "        - `figs_dict`:\n",
    "            - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try:\n",
    "    \n",
    "#     # Save the model\n",
    "#     model.save(FPATHS['modeling']['best_model_cnn_fpaths']['model_dir'], save_format='tf')\n",
    "\n",
    "# except Exception as e:\n",
    "#     display(f\"[i] Error saving model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPATHS['modeling']['best_cnn_fpaths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results dict:')\n",
    "print(results.keys())\n",
    "print(results['test'].keys())\n",
    "\n",
    "\n",
    "print('\\nFigs Dict')\n",
    "print(figs.keys())\n",
    "print(figs['test'].keys())\n",
    "\n",
    "def save_results(figs,model_key=None,FPATHS=None, results=None): \n",
    "                #  history_fpath=None, conf_matrix_fpath_train=None, \n",
    "                #  conf_matrix_fpath_test=None,\n",
    "                #  classification_report_fpath=None # not used yet\n",
    "                #  ):\n",
    "    \n",
    "    # if (model_key is not None) & (FPATHS is not None):\n",
    "    model_fpaths = FPATHS['modeling'][model_key]\n",
    "    history_fpath = model_fpaths.get('model_history_png',None)\n",
    "    conf_matrix_fpath_train = model_fpaths.get('model_confusion_matrix',None)\n",
    "    conf_matrix_fpath_test = model_fpaths.get('model_confusion_matrix',None)\n",
    "    \n",
    "    if 'history' in figs.keys():\n",
    "        if history_fpath is None:\n",
    "            history_fpath = FPATHS['results']['best_model_cnn_history']\n",
    "        figs['history'].savefig(history_fpath, dpi=300, bbox_inches='tight', transparent=False)\n",
    "        print(f\"\\n[i] Saved history plot to {history_fpath}\")\n",
    "\n",
    "    for split in ['train','test']:\n",
    "        if split in figs:\n",
    "            figs_split = figs[split]\n",
    "            \n",
    "            if 'confusion_matrix' in figs_split.keys():\n",
    "                conf_matrix_fpath_train\n",
    "            # if conf_matrix_fpath_train is None:\n",
    "                # conf_matrix_fpath_train = FPATHS['results']['best_model_cnn_confusion_matrix']\n",
    "                    \n",
    "                figs['confusion_matrix'].savefig(conf_matrix_fpath_train, dpi=300, bbox_inches='tight', transparent=False)\n",
    "                print(f\"\\n[i] Saved {split} confusion matrix plot to {conf_matrix_fpath_test}\") \n",
    "                \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_results(figs, history_fpath=FPATHS['results']['best_model_cnn_classification_report'], conf_matrix_fpath_train=None, \n",
    "#                  conf_matrix_fpath_test=None,\n",
    "#                  classification_report_fpath=None # not used yet\n",
    "#                  )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"finish writing save_results function above.\")\n",
    "# if OVERRIDE_TESTING:\n",
    "#     raise Exception(\"finish testing custom_evaluate_classification_network in eval function above.\")\n",
    "# # raise Exception(\"finish testing conf_matrix_text_kws in eval function above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1-LR: Adding LR Scheduling to Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Baseline model\n",
    "model = make_model(show_summary=False, \n",
    "                   use_schedule=True # Adding learning rate scheduling\n",
    "                   )\n",
    "history = model.fit(train_dataset,epochs = EPOCHS ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "\n",
    "results = custom_evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "                                                 target_names=label_lookup.values(),display_bar=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2 (Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_model2(name='CNN1',show_summary=False,use_schedule=False):\n",
    "    \n",
    "    model = models.Sequential(name=name)\n",
    "    # Using rescaling layer to scale pixel values\n",
    "    model.add(layers.Rescaling(1./255 , input_shape = (IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "    \n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16,  # How many filters you want to use\n",
    "            kernel_size=3, # size of each filter\n",
    "            # input_shape=input_shape,\n",
    "            padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "\n",
    "\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=32,#64,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            # input_shape=input_shape,\n",
    "            padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    \n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(\n",
    "        layers.Dense(len(label_lookup), activation=\"softmax\") )  \n",
    "    \n",
    "        \n",
    "    ## JMI:\n",
    "    if use_schedule:\n",
    "        lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=0.01, decay_steps=10000, decay_rate=0.95\n",
    "        )  # 0.9)\n",
    "        optimizer = optimizers.legacy.Adam(learning_rate=lr_schedule)\n",
    "    else:\n",
    "        optimizer = optimizers.legacy.Adam()#learning_rate=0.01)\n",
    "    \n",
    "    # ## Adding learning rate decay\n",
    "    # lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "    #     initial_learning_rate=0.01, decay_steps=10000, decay_rate=0.95\n",
    "    # )  # 0.9)\n",
    "    # optimizer = optimizers.legacy.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show model architecture\n",
    "model2 = make_model2(name=\"cnn1-fixed-lr\", show_summary=True, use_schedule=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model2 = make_model2(name=\"cnn1-fixed-lr\", show_summary=False, use_schedule=False)\n",
    "history2 = model2.fit(train_dataset, epochs = EPOCHS ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "# results_dict = cf.evaluate_classification_network(model2,X_test=test_dataset,history=history2, figsize=(15,15), \n",
    "# output_dict=True, target_names=label_lookup.values())\n",
    "# results_dict.keys()\n",
    "results_dict = custom_evaluate_classification_network(model2,X_test=test_dataset,history=history2, figsize=(20,20), \n",
    "                                                      target_names=label_lookup.values(),\n",
    "                                                      frame_include_macro_avg=False, frame_include_support=False,\n",
    "                                                      display_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding LR Scheduling with Exponential Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model2_lr = make_model2(use_schedule=True, show_summary=False, name=\"cnn1-scheduled-lr\")\n",
    "history_lr = model2_lr.fit(train_dataset,epochs = EPOCHS ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "# results_dict = cf.evaluate_classification_network(model2,X_test=test_dataset,history=history2, figsize=(15,15), output_dict=True, target_names=label_lookup.values())\n",
    "# results_dict.keys()\n",
    "results_dict = custom_evaluate_classification_network(model2_lr,X_test=test_dataset,history=history_lr, figsize=(20,20), \n",
    "                                                      target_names=label_lookup.values(),\n",
    "                                                      frame_include_macro_avg=False, frame_include_support=False,\n",
    "                                                      display_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Best Non-Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Model             |   Size (MB) | Top-1 Accuracy   | Top-5 Accuracy   | Parameters   | Depth   | Time (ms) per inference step (CPU)   | Time (ms) per inference step (GPU)   |\n",
    "|:------------------|------------:|:-----------------|:-----------------|:-------------|:--------|:-------------------------------------|:-------------------------------------|\n",
    "| **VGG16**             |      528    | 71.3%            | 90.1%            | 138.4M       | 16      | 69.5                                 | 4.2                                  |\n",
    "| **EfficientNetB0**    |       29    | 77.1%            | 93.3%            | 5.3M         | 132     | 46.0                                 | 4.9                                  |\n",
    "| **InceptionV3**       |       92    | 77.9%            | 93.7%            | 23.9M        | 189     | 42.2                                 | 6.9                                  |\n",
    "\n",
    "*Excerpt from Source: \"https://keras.io/api/applications/\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_HEIGHT,IMG_WIDTH,3)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading just the convolutional base\n",
    "vgg16_base = tf.keras.applications.VGG16(\n",
    "    include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    ")\n",
    "# Prevent layers from base_model from changing \n",
    "vgg16_base.trainable = False\n",
    "\n",
    "# Create the preprocessing lamdba layer\n",
    "# Create a lambda layer for the preprocess input function for the model\n",
    "lambda_layer_vgg16 = tf.keras.layers.Lambda(\n",
    "    tf.keras.applications.vgg16.preprocess_input, name=\"preprocess_input\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def make_vgg16_model(show_summary=False):\n",
    "    model = models.Sequential(name=\"VGG16\")\n",
    "    # Use input layer (lambda layer will handle rescaling).\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    ## Adding preprocessing lamabda layer\n",
    "    model.add(lambda_layer_vgg16)\n",
    "\n",
    "    # Add pretrained base\n",
    "    model.add(vgg16_base)\n",
    "\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ## Adding a Hidden Dense Layer\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(label_lookup.values()), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "        # loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Baseline model\n",
    "model_vgg = make_vgg16_model(show_summary=False, \n",
    "                \n",
    "                   )\n",
    "history = model_vgg.fit(train_dataset,epochs = EPOCHS ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "\n",
    "results = custom_evaluate_classification_network(model_vgg,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "                                                 target_names=label_lookup.values(),display_bar=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception('not ready for below')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download EfficientNet base\n",
    "efficientnet_base =tf.keras.applications.EfficientNetB0(include_top=False, \n",
    "                                                       input_shape=input_shape)\n",
    "efficientnet_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make it not-trainable\n",
    "efficientnet_base.trainable=False\n",
    "\n",
    "# add preprocessing lambda layer\n",
    "lambda_layer_efficient = tf.keras.layers.Lambda(tf.keras.applications.efficientnet.preprocess_input, \n",
    "                                      name='preprocess_input_enet')\n",
    "\n",
    "def make_efficientnet_model(show_summary=True):\n",
    "    model = models.Sequential(name=\"EfficientNetB0\")\n",
    "    # Use input layer (lambda layer will handle rescaling).\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    ## Adding preprocessing lamabda layer\n",
    "    model.add(lambda_layer_efficient)\n",
    "\n",
    "    # Add pretrained base\n",
    "    model.add(efficientnet_base)\n",
    "\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ## Adding a Hidden Dense Layer\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(label_lookup.values()), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "# vk.layered_view(efficientnet_base, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eff = make_efficientnet_model(show_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Baseline model\n",
    "model_eff = make_efficientnet_model(show_summary=False)\n",
    "history = model_eff.fit(train_dataset,epochs = EPOCHS ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "\n",
    "results = custom_evaluate_classification_network(model_eff,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "                                                 target_names=label_lookup.values(),display_bar=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eff.save(filepath=FPATHS['modeling']['best_transfer_fpaths']['model_dir'], save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üö® Need to change IMG_HEIGHT, IMG_WIDTH to 224 to use top of efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download EfficientNet base\n",
    "# efficientnet_full =tf.keras.applications.EfficientNetB0(include_top=True, \n",
    "#                                                        input_shape=input_shape)\n",
    "# efficientnet_full.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Fit and evaluate model with custom function\n",
    "# model2 = make_model2()\n",
    "# history2 = model2.fit(train_dataset,epochs = 100 ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "# results_dict = custom_evaluate_classification_network(model2,X_test=test_dataset,history=history2, figsize=(15,15), \n",
    "#                                                       target_names=label_lookup.values(),\n",
    "#                                                       as_frame=True, frame_include_macro_avg=False, frame_include_support=False,\n",
    "#                                                       display_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: Add LimeExplanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL = model_eff   #None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert test data to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# timing WITH converting classes\n",
    "y_test, y_hat_test, X_test = cf.get_true_pred_labels_images(BEST_MODEL,test_dataset,\n",
    "                                                         convert_y_for_sklearn=True)\n",
    "y_test[0], y_hat_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "y_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an image index to use/view\n",
    "i = 10\n",
    "\n",
    "# Show actual-sized image with keras\n",
    "display(array_to_img(X_test[i]))\n",
    "print(f\"True Label: {label_lookup[y_test[i]]}\")\n",
    "print(f\"Predicted: {label_lookup[y_hat_test[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LimeExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do:\n",
    "- Fix the comparison images below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Do not run below yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "from lime import lime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer(verbose=False)#,random_state=321)\n",
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label= label_lookup[y_test[i]]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the explanation object for the chosen\n",
    "explanation = explainer.explain_instance(X_test[i], # Convert image values to ints    \n",
    "                                         model.predict, # Prediction method/function\n",
    "                                         top_labels=1, # How many of the labels to explain [?]\n",
    "                                         hide_color=0, #\n",
    "                                         num_samples=1000,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stored original image\n",
    "plt.imshow(explanation.image)#.astype(int));\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation split image into \"segments\"\n",
    "plt.imshow(explanation.segments); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Segments\n",
    "np.unique(explanation.segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pros and cons\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                            positive_only=True, \n",
    "                                            num_features=5, \n",
    "                                            hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp, mask))\n",
    "plt.axis('off')\n",
    "plt.title('Segments that Positively Pushed Prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pros and cons\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                            negative_only=True, \n",
    "                                            positive_only=False,\n",
    "                                            num_features=5, \n",
    "                                            hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp, mask))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pros and cons\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                            negative_only=False, \n",
    "                                            positive_only=False,\n",
    "                                            num_features=5, \n",
    "                                            hide_rest=False)\n",
    "plt.imshow(mark_boundaries(temp, mask))\n",
    "plt.axis('off')\n",
    "plt.title(f'Segments that Pushed Prediction Towards (Green) or Away (Red) from {label}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_comparison(main_image, img, mask):\n",
    "    \"\"\"Adapted from Source:\n",
    "    https://coderzcolumn.com/tutorials/artificial-intelligence/lime-explain-keras-image-classification-network-predictions\"\"\"\n",
    "    fig,axes = plt.subplots(ncols=4,figsize=(15,5))\n",
    "\n",
    "    # show original image\n",
    "    ax = axes[0]\n",
    "    ax.imshow(main_image)#.astype(int))#, cmap=\"gray\");\n",
    "    ax.set_title(\"Original Image\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax =axes[1]\n",
    "    ax.imshow(img)#.astype(int));\n",
    "    ax.set_title(\"Image\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax = axes[2]\n",
    "    ax.imshow(mask);\n",
    "    ax.set_title(\"Mask\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax = axes[3]\n",
    "    ax.imshow(mark_boundaries(img,\n",
    "                              mask, color=(0,1,0)));\n",
    "    ax.set_title(\"Image+Mask Combined\");\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(X_test[i], temp, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining an Incorrect Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dojo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

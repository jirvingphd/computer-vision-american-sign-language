{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Sign Language - Computer Vision Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset: https://public.roboflow.com/object-detection/american-sign-language-letters\n",
    "- Example Task: https://towardsdatascience.com/sign-language-recognition-with-advanced-computer-vision-7b74f20f3442"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the seed for TensorFlow\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tf.__version__\n",
    "\n",
    "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# sys.path.append(os.path.abspath(\"../../\"))\n",
    "import ann_functions as af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the contents of data folder\n",
    "data_dir = \"./American Sign Language Letters.v1-v1.multiclass/\"\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of img file paths (ONLY, did not make recursuve so no folders)\n",
    "img_files = glob.glob(data_dir+\"**/*\")#, recursive=True)\n",
    "len(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview an example image (at full size)\n",
    "img_loaded = load_img(img_files[0])\n",
    "img_data = img_to_array(img_loaded)\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ›ï¸ Project Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set project-wide parameters\n",
    "# # Saving image params as vars for reuse\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "TRAIN_SPLIT = 0.7  # Proportion of data for training\n",
    "VAL_SPLIT = 0.15  # Proportion of data for validation (remaining will be for test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare CSV of Filenames + Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "csv_path = os.path.join(data_dir,\"train\",\"_classes.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df = df.convert_dtypes()\n",
    "# df['filename'] = df['filename']\n",
    "df = df.set_index('filename')\n",
    "df = df.astype(float)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine label columns into single column\n",
    "df.loc[:,'label'] = df.apply(lambda x: x.idxmax(), axis=1)\n",
    "display(df.head(2))\n",
    "df['label'].value_counts(1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = df.drop(columns=['filename', 'label'], errors='ignore').sum()\n",
    "n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = sorted(df.drop(columns=['filename','label'], errors='ignore').columns)\n",
    "label_lookup = {i:label for i,label in enumerate(label_cols)}\n",
    "label_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the filepaths and labels\n",
    "df = df.reset_index(drop=False)\n",
    "\n",
    "df['filepath'] = df.loc[:,'filename'].astype(str).map(lambda x: os.path.join(data_dir, \"train/\", x)).values\n",
    "filepaths = df['filepath']\n",
    "\n",
    "labels = df[label_cols].astype(float).values\n",
    "filepaths[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_exist = np.array([os.path.exists(f) for f in filepaths])\n",
    "files_exist.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_img(filepaths[0]))\n",
    "print(f\"Letter: {label_lookup[np.argmax(labels[0])]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create a TensorFlow dataset from the image paths and labels\n",
    "# def load_image(image_path, label, img_height=128, img_width=128):\n",
    "#     target_size=(img_height, img_width)\n",
    "#     image = load_img(image_path, target_size=target_size)\n",
    "#     image = img_to_array(image)\n",
    "#     image = image / 255.0  # Normalize the image\n",
    "#     return image, label\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_image(filename, label, img_height=128, img_width=128):\n",
    "    img = tf.io.read_file(filename)\n",
    "    # img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img.set_shape([None, None, 3])  # Explicitly set the shape\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    # img = img / 255.0  # Normalize the image\n",
    "    return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_img, ex_label = load_image(filepaths[0], labels[0])\n",
    "print(ex_img.shape)\n",
    "display(array_to_img(ex_img))\n",
    "print(f\"Label: {ex_label}\")\n",
    "print(f\"Label: {label_lookup[np.argmax(ex_label)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = df[['filepath', 'label']]\n",
    "eda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Showing example of each letter\n",
    "# label_lookup.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = sorted(df['label'].unique())\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax = sns.countplot(data=eda_df, x='label',order=label_lookup.values(),\n",
    "              hue='label', dodge=False,palette=sns.color_palette(\"icefire\",n_colors=len(label_lookup)),\n",
    "              ax=ax)\n",
    "ax.set(title=\"Distribution of Labels\", xlabel=\"Letter\", ylabel=\"Count\")\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "fig.savefig(\"images/label_dist.png\", dpi=300, bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Example of Each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot example of each letter\n",
    "import os\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "\n",
    "ncols = 6\n",
    "unique_labels = sorted(eda_df['label'].unique())\n",
    "nrows = len(unique_labels)//ncols + 1\n",
    "\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(15,15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    fpath = eda_df.loc[ eda_df['label']==label,'filepath'].sample(1).values[0]\n",
    "    \n",
    "    loaded = plt.imread(fpath)\n",
    "    axes[i].imshow(loaded)\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "\n",
    "# remove unused axes\n",
    "axes_labels_diff =  len(axes) - len(unique_labels)\n",
    "\n",
    "if axes_labels_diff>0:\n",
    "    for ax in axes[-axes_labels_diff:]:\n",
    "        \n",
    "        # difference = len(axes)\n",
    "        fig.delaxes(ax=ax)   \n",
    "        \n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"images/eda_example_letters.png\", dpi=300, bbox_inches='tight', transparent=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Plot example of each letter\n",
    "# import os\n",
    "# os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# ncols = 6\n",
    "# unique_labels = sorted(eda_df['label'].unique())\n",
    "# nrows = len(unique_labels)//ncols + 1\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(15,15))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "\n",
    "# for i, label in enumerate(unique_labels):\n",
    "#     fpath = eda_df.loc[ eda_df['label']==label,'filepath'].sample(1).values[0]\n",
    "    \n",
    "#     loaded = plt.imread(fpath)\n",
    "#     axes[i].imshow(loaded)\n",
    "#     axes[i].set_title(label)\n",
    "#     axes[i].axis('off')\n",
    "    \n",
    "\n",
    "# # remove unused axes\n",
    "# axes_labels_diff =  len(axes) - len(unique_labels)\n",
    "\n",
    "# if axes_labels_diff>0:\n",
    "#     for ax in axes[-axes_labels_diff:]:\n",
    "        \n",
    "#         # difference = len(axes)\n",
    "#         fig.delaxes(ax=ax)   \n",
    "        \n",
    "# fig.tight_layout()\n",
    "\n",
    "# fig.savefig(\"images/eda_example_letters.png\", dpi=300, bbox_inches='tight', transparent=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[:,'label'] = df.loc[:,label_cols].apply(lambda x: x.idxmax(), axis=1)\n",
    "# df['label'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = np.array(image_paths)\n",
    "# labels = np.array(labels)\n",
    "# image_paths.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Train/Test/Val Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_image(image_paths[0], labels[0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(buffer_size=len(dataset), reshuffle_each_iteration=False)\n",
    "\n",
    "dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Map the load_image function to the dataseta\n",
    "dataset = dataset.map(lambda x,y: load_image(x,y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine split sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(TRAIN_SPLIT * total_size)\n",
    "val_size = int(VAL_SPLIT * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "print(f\"{train_size=}, {test_size=}, {val_size=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "test_dataset = dataset.skip(train_size + val_size)\n",
    "\n",
    "# # Cache the datset for faster access\n",
    "# train_dataset = train_dataset.cache()\n",
    "# val_dataset = val_dataset.cache()\n",
    "# test_dataset = test_dataset.cache() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and prefetch the datasets\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle the trainin data\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_dataset.cardinality(), \n",
    "                                      reshuffle_each_iteration=True) # DOUBLE CHECK BATCH_SIZE * 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the datasets\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(f\"Train batch - images: {images.shape}, labels: {labels.shape}\")\n",
    "    \n",
    "for images, labels in val_dataset.take(1):\n",
    "    print(f\"Val batch - images: {images.shape}, labels: {labels.shape}\")\n",
    "    \n",
    "    \n",
    "for images, labels in test_dataset.take(1):\n",
    "    print(f\"Test batch - images: {images.shape}, labels: {labels.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model (From towardsdatascience blog)\n",
    "- from https://towardsdatascience.com/sign-language-recognition-with-advanced-computer-vision-7b74f20f3442\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moedl from https://towardsdatascience.com/sign-language-recognition-with-advanced-computer-vision-7b74f20f3442\n",
    "# from tensorflow\n",
    "def make_model(name='towards-data-science',show_summary=False, use_schedule=False):\n",
    "    model = models.Sequential(name=name)\n",
    "    model.add(layers.Rescaling(1./255 , input_shape = (IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "    \n",
    "    model.add(layers.Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' ))#, input_shape = (28,28,1)))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    model.add(layers.Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    model.add(layers.Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "    \n",
    "    # Final layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units = 512 , activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(units = len(label_lookup   ) , activation = 'softmax'))\n",
    "    \n",
    "    \n",
    "    ## JMI:\n",
    "    if use_schedule:\n",
    "        lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=0.01, decay_steps=10000, decay_rate=0.95\n",
    "        )  # 0.9)\n",
    "        optimizer = optimizers.legacy.Adam(learning_rate=lr_schedule)\n",
    "    else:\n",
    "        optimizer = optimizers.legacy.Adam()#learning_rate=0.01)\n",
    "        \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    # model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Demonstrate model architecture\n",
    "model = make_model(show_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `def get_callbacks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_callbacks(monitor='val_accuracy', patience=10, start_from_epoch=5, restore_best_weights=False):\n",
    "    \"\"\"\n",
    "    Returns a list of callbacks for training a model.\n",
    "\n",
    "    Parameters:\n",
    "    - monitor (str): The metric to monitor. Default is 'val_accuracy'.\n",
    "    - patience (int): The number of epochs with no improvement after which training will be stopped. Default is 15.\n",
    "    - start_from_epoch (int): The epoch from which to start counting the patience. Default is 3.\n",
    "    - restore_best_weights (bool): Whether to restore the weights of the best epoch. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - callbacks (list): A list of callbacks to be used during model training.\n",
    "    \"\"\"\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,start_from_epoch=start_from_epoch,\n",
    "                                                      monitor=monitor,\n",
    "                                                      restore_best_weights=restore_best_weights, verbose=1)\n",
    "    return [early_stopping]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "# # Baseline model\n",
    "# model = make_model(show_summary=False, use_schedule=False)\n",
    "# history = model.fit(train_dataset,epochs = 100,#0 ,\n",
    "#                     validation_data = val_dataset, callbacks=get_callbacks())\n",
    "# af.evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "#                                    target_names=label_lookup.values());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Evaluation to Handle Large # Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With 26 classes, it is difficult to scan the performance for each class visually. Adding code to convert results to a datafarme and use pandas styling to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "# results_dict = af.evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15), output_dict=True, target_names=label_lookup.values())\n",
    "# results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "# results_dict['test'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "# from IPython.display import clear_output\n",
    "# results_dict = af.evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15), output_dict=True, target_names=label_lookup.values())\n",
    "# clear_output()\n",
    "# results = results_dict['test'].copy()\n",
    "# try:\n",
    "#     accuracy = results.pop('accuracy')\n",
    "#     macro_avg = results.pop('macro avg')\n",
    "#     _ = results.pop('weighted avg')\n",
    "# except Exception as e:\n",
    "#     display(e)\n",
    "\n",
    "# results_df = pd.DataFrame(results).T\n",
    "# results_df['support'] = results_df['support'].astype(int)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "\n",
    "# overall_results = pd.DataFrame(macro_avg, index=['macro avg'])#.T\n",
    "# overall_results.insert(0,'accuracy',accuracy)\n",
    "# overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "\n",
    "# accuracy, macro_avg\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_df(results_dict, results_key='test', \n",
    "                   average_rowname= 'macro avg',\n",
    "                   include_support = True,\n",
    "                   include_macro_avg=True):\n",
    "    \"\"\"\n",
    "    Convert a results dictionary into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dict (dict): A dictionary containing the results.\n",
    "    - results_key (str): The key in the dictionary that contains the results. Default is 'test'.\n",
    "    - average_rowname (str): The name of the row that represents the average. Default is 'macro avg'.\n",
    "    - include_support (bool): Whether to include the 'support' column in the DataFrame. Default is True.\n",
    "    - include_macro_avg (bool): Whether to include the 'macro avg' row in the DataFrame. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - results_df (pandas DataFrame): A DataFrame containing the results.\n",
    "\n",
    "    \"\"\"\n",
    "    results = results_dict[results_key].copy()\n",
    "    \n",
    "    # Remove accuracy and macro avg from results\n",
    "    accuracy = results.pop('accuracy')\n",
    "    macro_avg = results.pop('macro avg')\n",
    "    _ = results.pop('weighted avg')\n",
    "    \n",
    "    # Create DataFrames\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    \n",
    "    if include_macro_avg:\n",
    "        overall_results = pd.DataFrame(macro_avg, index=[average_rowname])#.T\n",
    "    \n",
    "        ## Concatenate the overall results to the results_df\n",
    "        results_df = pd.concat([results_df, overall_results],axis=0)\n",
    "        results_df.loc[average_rowname,'accuracy'] = accuracy\n",
    "    \n",
    "    # Recast support as int\n",
    "    results_df['support'] = results_df['support'].astype(int)\n",
    "\n",
    "    # Move the support column to the end\n",
    "    # results_df = results_df[ results_df.drop(columns='support').columns.tolist() + ['support']]\n",
    "    \n",
    "    if not include_support:\n",
    "        results_df = results_df.drop(columns='support')\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "\n",
    "# results_dict = af.evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15), output_dict=True, target_names=label_lookup.values())\n",
    "# results_test = get_results_df(results_dict)\n",
    "# results_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "\n",
    "# results_test = get_results_df(results_dict, include_macro_avg=False, include_support=False)\n",
    "# results_test.style.bar(color='#5fba7d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "\n",
    "# results_test.style.background_gradient(cmap='Greens', vmax=1,vmin=0, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added new kwarg to evaluate_classification_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scratch Code to create function\n",
    "\n",
    "# results_dict = af.evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "#                                                   output_dict=True, target_names=label_lookup.values(),\n",
    "#                                                   as_frame=True,\n",
    "#                                                   frame_include_macro_avg=False,frame_include_support=False)\n",
    "# # results_test = get_results_df(results_dict)\n",
    "# # results_test\n",
    "# results_dict['test'].style.bar(color='#5fba7d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated custom Evaluation Function (for Notebook use only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluate_classification_network(model, X_test, history=None, figsize=(15,15), target_names=None,\n",
    "                                             as_frame=True, frame_include_macro_avg=False, frame_include_support=False,\n",
    "                                             display_bar=True):\n",
    "    \"\"\"\n",
    "    Evaluate a classification model on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained classification model.\n",
    "    - X_test: The test dataset.\n",
    "    - history: The training history of the model (optional).\n",
    "    - figsize: The size of the figure for plotting the evaluation results (default: (15, 15)).\n",
    "    - target_names: The names of the target classes (default: None).\n",
    "    - as_frame: Whether to return the evaluation results as a pandas DataFrame (default: True).\n",
    "    - frame_include_macro_avg: Whether to include macro average metrics in the DataFrame (default: False).\n",
    "    - frame_include_support: Whether to include support values in the DataFrame (default: False).\n",
    "    - display_bar: Whether to display the evaluation results as a styled bar chart (default: True).\n",
    "\n",
    "    Returns:\n",
    "    - results_dict: A dictionary containing the evaluation results.\n",
    "    \"\"\"\n",
    "    if target_names is None:\n",
    "        # label_lookup is in the global scope\n",
    "        target_names = label_lookup.values()\n",
    "        \n",
    "    results_dict = af.evaluate_classification_network(model,\n",
    "                                                      X_test=X_test,history=history, figsize=figsize,\n",
    "                                                  output_dict=True, target_names=target_names,#label_lookup.values(),\n",
    "                                                  as_frame=True,\n",
    "                                                    frame_include_macro_avg=frame_include_macro_avg,\n",
    "                                                    frame_include_support=frame_include_support)\n",
    "    # results_test = get_results_df(results_dict)\n",
    "    # results_test\n",
    "    if display_bar:\n",
    "        display(results_dict['test'].style.bar(color='#5fba7d'))\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dict['test'].style.bar(color='#5fba7d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (with New Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show model architecture\n",
    "model = make_model(show_summary=True, use_schedule=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Baseline model\n",
    "model = make_model(show_summary=False, use_schedule=False)\n",
    "history = model.fit(train_dataset,epochs = 100 ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "# results_dict = af.evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "#                                                   output_dict=True, target_names=label_lookup.values(),\n",
    "#                                                   as_frame=True,\n",
    "#                                                   frame_include_macro_avg=False,frame_include_support=False)\n",
    "results = custom_evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "                                                 target_names=label_lookup.values(),display_bar=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1-LR: Adding LR Scheduling to Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Baseline model\n",
    "model = make_model(show_summary=False, \n",
    "                   use_schedule=True # Adding learning rate scheduling\n",
    "                   )\n",
    "history = model.fit(train_dataset,epochs = 100 ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "\n",
    "results = custom_evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "                                                 target_names=label_lookup.values(),display_bar=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2 (custom from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_model2(name='CNN1',show_summary=False):\n",
    "    \n",
    "    model = models.Sequential(name=name)\n",
    "    # Using rescaling layer to scale pixel values\n",
    "    model.add(layers.Rescaling(1./255 , input_shape = (IMG_HEIGHT,IMG_WIDTH,3)))\n",
    "    \n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16,  # How many filters you want to use\n",
    "            kernel_size=3, # size of each filter\n",
    "            # input_shape=input_shape,\n",
    "            padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "\n",
    "\n",
    "    # Convolutional layer\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=32,#64,  # How many filters you want to use\n",
    "            kernel_size=3,  # size of each filter\n",
    "            # input_shape=input_shape,\n",
    "            padding='same')) \n",
    "    # Pooling layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))  # Size of pooling\n",
    "    \n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(\n",
    "        layers.Dense(len(label_lookup), activation=\"softmax\") )  \n",
    "    ## Adding learning rate decay\n",
    "    lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.01, decay_steps=10000, decay_rate=0.95\n",
    "    )  # 0.9)\n",
    "    optimizer = optimizers.legacy.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show model architecture\n",
    "model2 = make_model2(show_summary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model2 = make_model2()\n",
    "history2 = model2.fit(train_dataset,epochs = 100 ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "# results_dict = af.evaluate_classification_network(model2,X_test=test_dataset,history=history2, figsize=(15,15), output_dict=True, target_names=label_lookup.values())\n",
    "# results_dict.keys()\n",
    "results_dict = custom_evaluate_classification_network(model2,X_test=test_dataset,history=history2, figsize=(20,20), \n",
    "                                                      target_names=label_lookup.values(),\n",
    "                                                      as_frame=True, frame_include_macro_avg=False, frame_include_support=False,\n",
    "                                                      display_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Model             |   Size (MB) | Top-1 Accuracy   | Top-5 Accuracy   | Parameters   | Depth   | Time (ms) per inference step (CPU)   | Time (ms) per inference step (GPU)   |\n",
    "|:------------------|------------:|:-----------------|:-----------------|:-------------|:--------|:-------------------------------------|:-------------------------------------|\n",
    "| **VGG16**             |      528    | 71.3%            | 90.1%            | 138.4M       | 16      | 69.5                                 | 4.2                                  |\n",
    "| **EfficientNetB0**    |       29    | 77.1%            | 93.3%            | 5.3M         | 132     | 46.0                                 | 4.9                                  |\n",
    "| **InceptionV3**       |       92    | 77.9%            | 93.7%            | 23.9M        | 189     | 42.2                                 | 6.9                                  |\n",
    "\n",
    "*Excerpt from Source: \"https://keras.io/api/applications/\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_HEIGHT,IMG_WIDTH,3)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading just the convolutional base\n",
    "vgg16_base = tf.keras.applications.VGG16(\n",
    "    include_top=False, weights=\"imagenet\", input_shape=input_shape\n",
    ")\n",
    "# Prevent layers from base_model from changing \n",
    "vgg16_base.trainable = False\n",
    "\n",
    "# Create the preprocessing lamdba layer\n",
    "# Create a lambda layer for the preprocess input function for the model\n",
    "lambda_layer_vgg16 = tf.keras.layers.Lambda(\n",
    "    tf.keras.applications.vgg16.preprocess_input, name=\"preprocess_input\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def make_vgg16_model(show_summary=False):\n",
    "    model = models.Sequential(name=\"VGG16\")\n",
    "    # Use input layer (lambda layer will handle rescaling).\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    ## Adding preprocessing lamabda layer\n",
    "    model.add(lambda_layer_vgg16)\n",
    "\n",
    "    # Add pretrained base\n",
    "    model.add(vgg16_base)\n",
    "\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ## Adding a Hidden Dense Layer\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(label_lookup.values()), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Baseline model\n",
    "model = make_vgg16_model(show_summary=False, \n",
    "                   use_schedule=True # Adding learning rate scheduling\n",
    "                   )\n",
    "history = model.fit(train_dataset,epochs = 100 ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "\n",
    "results = custom_evaluate_classification_network(model,X_test=test_dataset,history=history, figsize=(15,15),\n",
    "                                                 target_names=label_lookup.values(),display_bar=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('not ready for below')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download EfficientNet base\n",
    "efficientnet_base =tf.keras.applications.EfficientNetB0(include_top=False, \n",
    "                                                       input_shape=input_shape)\n",
    "\n",
    "# Make it not-trainable\n",
    "efficientnet_base.trainable=False\n",
    "\n",
    "# add preprocessing lambda layer\n",
    "lambda_layer_efficient = tf.keras.layers.Lambda(tf.keras.applications.efficientnet.preprocess_input, \n",
    "                                      name='preprocess_input_enet')\n",
    "\n",
    "def build_efficientnet_model():\n",
    "    model = models.Sequential(name=\"EfficientNetB0\")\n",
    "    # Use input layer (lambda layer will handle rescaling).\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    ## Adding preprocessing lamabda layer\n",
    "    model.add(lambda_layer_efficient)\n",
    "\n",
    "    # Add pretrained base\n",
    "    model.add(efficientnet_base)\n",
    "\n",
    "    # Flattening layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ## Adding a Hidden Dense Layer\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(len(label_lookup.values()), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# vk.layered_view(efficientnet_base, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Fit and evaluate model with custom function\n",
    "# model2 = make_model2()\n",
    "# history2 = model2.fit(train_dataset,epochs = 100 ,validation_data = val_dataset, callbacks=get_callbacks())\n",
    "# results_dict = custom_evaluate_classification_network(model2,X_test=test_dataset,history=history2, figsize=(15,15), \n",
    "#                                                       target_names=label_lookup.values(),\n",
    "#                                                       as_frame=True, frame_include_macro_avg=False, frame_include_support=False,\n",
    "#                                                       display_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: Add LimeExplanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert test data to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# timing WITH converting classes\n",
    "y_test, y_hat_test, X_test = af.get_true_pred_labels_images(BEST_MODEL,test_dataset,\n",
    "                                                         convert_y_for_sklearn=True)\n",
    "y_test[0], y_hat_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an image index to use/view\n",
    "i = 10\n",
    "\n",
    "# Show actual-sized image with keras\n",
    "display(array_to_img(X_test[i]))\n",
    "print(f\"True Label: {label_lookup[y_test[i]]}\")\n",
    "print(f\"Predicted: {label_lookup[y_hat_test[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LimeExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "from lime import lime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer(verbose=False)#,random_state=321)\n",
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the explanation object for the chosen\n",
    "explanation = explainer.explain_instance(X_test[i], # Convert image values to ints    \n",
    "                                         model.predict, # Prediction method/function\n",
    "                                         top_labels=1, # How many of the labels to explain [?]\n",
    "                                         hide_color=0, #\n",
    "                                         num_samples=1000,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stored original image\n",
    "plt.imshow(explanation.image)#.astype(int));\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation split image into \"segments\"\n",
    "plt.imshow(explanation.segments); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Segments\n",
    "np.unique(explanation.segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pros and cons\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                            positive_only=True, \n",
    "                                            num_features=5, \n",
    "                                            hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp, mask))\n",
    "plt.axis('off')\n",
    "plt.title('Segments that Positively Pushed Prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pros and cons\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                            negative_only=True, \n",
    "                                            positive_only=False,\n",
    "                                            num_features=5, \n",
    "                                            hide_rest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pros and cons\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                            negative_only=False, \n",
    "                                            positive_only=False,\n",
    "                                            num_features=5, \n",
    "                                            hide_rest=False)\n",
    "plt.imshow(mark_boundaries(temp, mask))\n",
    "plt.axis('off')\n",
    "plt.title(f'Segments that Pushed Prediction Towards (Green) or Away (Red) from {label}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_comparison(main_image, img, mask):\n",
    "    \"\"\"Adapted from Source:\n",
    "    https://coderzcolumn.com/tutorials/artificial-intelligence/lime-explain-keras-image-classification-network-predictions\"\"\"\n",
    "    fig,axes = plt.subplots(ncols=4,figsize=(15,5))\n",
    "\n",
    "    # show original image\n",
    "    ax = axes[0]\n",
    "    ax.imshow(main_image)#.astype(int))#, cmap=\"gray\");\n",
    "    ax.set_title(\"Original Image\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax =axes[1]\n",
    "    ax.imshow(img)#.astype(int));\n",
    "    ax.set_title(\"Image\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax = axes[2]\n",
    "    ax.imshow(mask);\n",
    "    ax.set_title(\"Mask\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax = axes[3]\n",
    "    ax.imshow(mark_boundaries(img,\n",
    "                              mask, color=(0,1,0)));\n",
    "    ax.set_title(\"Image+Mask Combined\");\n",
    "    ax.axis('off')\n",
    "    fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(X_test[i], temp, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining an Incorrect Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dojo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
